import os, sys
import json
from flask import Flask, request, jsonify
from llama_cpp import Llama

PROJECT_ROOT = os.getcwd()
MODEL_PATH = os.path.join(PROJECT_ROOT, 'models', 'Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf')
print(f'MODEL_PATH = {MODEL_PATH}')

# ëª¨ë¸ ë¡œë“œ (ìµœëŒ€ 1íšŒë§Œ!)
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=4096,
    n_threads=12,
    n_gpu_layers=64
)

def create_app():
    app = Flask(__name__)

    # ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ì €ì¥ (ê°„ë‹¨í•˜ê²Œ ë©”ëª¨ë¦¬ dictë¡œ)
    session_histories = {}

    def messages_to_prompt(messages):
        prompt = ""
        for msg in messages:
            prompt += f"<|{msg['role']}|>\n{msg['content']}\n"
        prompt += "<|assistant|>\n"
        return prompt

    def prune_history(messages, max_tokens=3000):
        while True:
            prompt_str = messages_to_prompt(messages)
            try:
                tokens = llm.tokenize(prompt_str.encode("utf-8"))
            except Exception as e:
                print("âš ï¸ í† í¬ë‚˜ì´ì¦ˆ ì‹¤íŒ¨:", e)
                break

            print(f"ğŸ§® í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: {len(messages)} / í† í° ìˆ˜: {len(tokens)}")
            if len(tokens) <= max_tokens:
                break

            # system ë©”ì‹œì§€ ì œì™¸í•˜ê³  ê°€ì¥ ì˜¤ë˜ëœ user/assistant í˜ì–´ ì‚­ì œ
            for i, msg in enumerate(messages):
                if msg["role"] != "system":
                    messages = messages[i+2:]
                    break

        return messages

    @app.route("/chat", methods=["POST"])
    def chat():
        print("âœ… ìš”ì²­ ë„ì°©í•¨")
        print("Headers:", dict(request.headers))
        print("Raw Data:", request.data)

        try:
            data = json.loads(request.data.decode("utf-8"))
        except Exception as e:
            return jsonify({"error": f"JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}"}), 400

        print("Parsed JSON:", data)

        session_id = data.get("session_id", "default")
        user_msg = data.get("message")
        if not isinstance(user_msg, str):
            return jsonify({"error": "message í•„ë“œëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤."}), 400

        history = session_histories.get(session_id, [])
        if not history:
            history.append({"role": "system", "content": "You are report printer. just print only formatted text."})
        history.append({"role": "user", "content": user_msg})
        history = prune_history(history)

        # ëª¨ë¸ í˜¸ì¶œ
        resp = llm.create_chat_completion(
            messages=history,
            max_tokens=4096,
            temperature=0.7,
            top_p=0.95
        )
        assistant_msg = resp["choices"][0]["message"]["content"]

        history.append({"role": "assistant", "content": assistant_msg})
        session_histories[session_id] = history
        
        # âœ… íˆìŠ¤í† ë¦¬ ì¶œë ¥
        print("ğŸ’¬ ëª¨ë¸ì— ë„˜ê¸°ëŠ” íˆìŠ¤í† ë¦¬:")
        for m in history:
            role = m['role']
            content = m['content'].replace("\n", "\\n")[:100]
            print(f" - {role}: {content}")

        # âœ… í”„ë¡¬í”„íŠ¸ë¡œ ì–´ë–»ê²Œ ë³´ì´ëŠ”ì§€ ì¶œë ¥
        print("ğŸ“ Chat Prompt Preview:")
        print(messages_to_prompt(history))

        return jsonify({"response": assistant_msg})
    
    return app
